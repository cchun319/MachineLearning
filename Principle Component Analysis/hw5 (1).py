# -*- coding: utf-8 -*-
"""hw5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-ZcmCXaYDZIrLZTnrCNK1d6dCmL3zBO

# Homework 5: Coding

**Due Monday October 28th, 11:59pm.**

**This is an individual assignment.**

**Submit hw5.py file to Gradescope (you may submit as many times as you'd like before the deadline).**
"""

"""
Import libraries that you might require.

DON'T comment out these imports when submitting your final hw5.py file.
"""

import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import sklearn.model_selection as ms
from sklearn.datasets import load_wine
from sklearn.datasets import fetch_lfw_people
from sklearn.preprocessing import StandardScaler

"""# Question 3: Simple PCA

In this assignment, we will implement Principal Component Analysis and perform it on a simple 2-dimensional dataset. One way to perform PCA is by doing an eigenvalue decomposition of a data covariance matrix, which we will implement here.

## PCA Implementation

First fill out these helper functions. **You are allowed to use numpy functions as needed**:
"""

def get_cov_mat(X):
  """
  Returns a covariance matrix of the given input matrix
  
  Args:
      X ((n,p) np.ndarray): The input data
  Returns:
      X_cov ((p,p) np.ndarray): The input data
  """  
  
  p = X.shape[1]  
  n = X.shape[0]
  # <---- Start your code here ----->
  # mean_x = np.mean(X, 0)
  
  # X_d = X - mean_x

  # X_cov = X_d.T @ X_d

  X_cov = X.T @ X / n

  # <---- End your code here ----->
  
  # confirm the shape of your output
  assert X_cov.shape == (p,p)
  
  return X_cov


def get_eig(cov_mat):
  """
  Returns an eigenvectors and eigenvalues in sorted order by eigenvalue
  given input covariance matrix
  
  Args:
      cov_mat ((p,p) np.ndarray): The input covariance matrix
  Returns:
      eig_vals_sorted ((p,) np.ndarray): Eigenvalues in decreasing order
      eig_vecs_sorted ((p,p) np.ndarray): Eigenvectors corresponding to the 
                                          sorted eigenvalues, eig_vecs_sorted[:, i]
                                          corresponds to the ith vector
                                          (i.e. each column is an eigenvector)
  """  
  
  p = cov_mat.shape[1]
  eig_vals_sorted = np.zeros(p)
  eig_vecs_sorted = np.zeros((p,p))
  # <---- Start your code here ----->

  eig_val, eig_vec = np.linalg.eig(cov_mat) # QV-1QT

  eig_vecs_sorted = eig_vec.T
  eig_vals_sorted = eig_val
  # <---- End your code here ----->  
  
  # confirm the shapes of your output
  assert eig_vals_sorted.shape == (p,)  
  assert eig_vecs_sorted.shape == (p,p)  
    
  return eig_vals_sorted, eig_vecs_sorted.T
  
    
def get_projection(X, principal_components):
  """
  Returns the projection of X onto the principal components
  
  Args:
      X ((n,p) np.ndarray): The input data
      principal_components ((p,p) np.ndarray): Principal components of data
  Returns:
      projection ((n,p) np.ndarray): The projection of X onto the components
  """  

  n = X.shape[0]
  p = X.shape[1]

  projection = np.zeros((n,p))

  
  # <---- Start code here ----->
  # mean_x = np.mean(X, 0)
  # X_d = X - mean_x
  # projection = X_d @ principal_components
  
  projection = X @ principal_components


  # <---- End code here ----->  

  # confirm the shape of your output
  assert projection.shape == (n,p)

  return projection

"""Next, fill out this main function for PCA using the helper functions above"""

def pca(X):
  """
  Performs PCA on dataset X.
  
  Args:
    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by
                          p (number of features) matrix                 
  
  Returns:
    projected ((n, p) np.ndarray): Samples projected on reduced dimensions
    values ((p,) np.ndarray): Singular Values
    components ((p, p) np.ndarray): Principal Components (also eigenvectors)
  """

  n = X.shape[0]
  p = X.shape[1]

  projection = np.zeros((n,p))
  components = np.zeros((p,p))
  values = np.zeros(p)

  # <---- Start code here ----->  
  X_cov = get_cov_mat(X)
  eig_val, eig_v  = get_eig(X_cov)
  values = np.sqrt(eig_val)
  components = eig_v
  projection = get_projection(X, eig_v)
  # <---- End code here ----->  

  # confirm the shape of your outputs
  assert projection.shape == (n,p)
  assert values.shape == (p,)
  assert components.shape == (p,p)

  return projection, components, values

"""
Load data (Two features from Sk-Learn Wine Dataset).

Please remember to comment out this code before downloading your .py file.
"""

# data = load_wine().data[:, [0, 12]]
# # X = data
# X = StandardScaler().fit_transform(data)

"""Call your pca function on sample 2-dimensional set, centering X on its mean first.

### 3.1 Answer on LaTeX
What can you say about the relationship between the first principal component and the second?
"""

"""
Please remember to comment out this code before downloading your .py file.
"""

# X_mean = np.mean(X, axis=0)
# print(X_mean)

# projected, components, values = pca(X - X_mean)
# p1 = components[:,0]
# print(p1)
# p2 = components[:,1]
# print(p2)

# print(components)
# print(values)

"""### 3.2 Answer on LaTeX
Now plot the given points (with both axis in same scale) as well as the lines representing the principal components in original space, with x1 in the x axis and x2 in the y axis. Paste the graph and please describe how the principal components relate to the points.
"""

"""
Please remember to comment out this code before downloading your .py file.
"""

# mean_vec = np.mean(X, axis=0)

# p1 = p1.reshape((len(p1), 1))
# p2 = p2.reshape((len(p2), 1))
# print(p2)
# p1_ = (X - X_mean) @ p1
# p2_ = (X - X_mean) @ p2

# p1_ = p1_.reshape((len(p1_), 1))
# p2_ = p2_.reshape((len(p2_), 1))

# plt.arrow(mean_vec[0], mean_vec[1], components[0, 0], components[1, 0], ec='red')
# plt.arrow(mean_vec[0], mean_vec[1], components[0, 1], components[1, 1], ec='red')
# plt.axis('equal')
# plt.scatter(X[:, 0], X[:, 1])

"""### 3.3 Answer on LaTeX
Now plot the given points (with both axis in same scale) in principal component space, with x-axis representing the projection of the first component and the y-axis representing the component on the second. Paste the graph.
1. Explain how the graph of points on principal component space relates to the graph of points on original space above.
1. Explain the difference in distribution of points projected on the first component vs. projected on the second.
"""

"""
Please remember to comment out this code before downloading your .py file.
# """
# plt.axis('equal')
# plt.scatter(projected[:, 0] , projected[:, 1])
# plt.scatter(X[:, 0], X[:, 1])
# rec = projected @ components.T
# # plt.scatter(rec[:, 0], rec[:, 1], marker='+')

# p1_rec = p1_ @ p1.T # p1_ = X@p1 , p1_rec = X@p1@p1.T
# p2_rec = p2_ @ p2.T

# print(p1.T @ p2)
# plt.ylim(-2, 3.5)
# plt.xlim(-5, 3)
# plt.scatter(X[:, 0], X[:, 1])
# plt.scatter(p1_rec[:, 0] + X_mean[0], p1_rec[:, 1] + X_mean[1], marker = '+')
# plt.scatter(p2_rec[:, 0] + X_mean[0], p2_rec[:, 1] + X_mean[1], marker = '*')

"""### 3.4 Answer on LaTeX
Now plot the given points (with both axis in same scale) in principal component space, with x-axis representing the projection of the first component and the y-axis representing the component on the second. Paste the graph.
1. Explain how the graph of points on principal component space relates to the graph of points on original space above.
1. Explain the difference in distribution of points projected on the first component vs. projected on the second.
"""

from sklearn.datasets import load_digits
from sklearn.preprocessing import scale

digits = load_digits()
digits.data.shape

x = digits.data#convert the data into a numpy array
x = scale(x);

print(x.shape)
x_mean = np.mean(x, axis=0)

projected_x, components_x, values_x = pca(x - x_mean)

p1_x = components_x[:,0]
p2_x = components_x[:,1]
p3_x = components_x[:,0:2]


p1_x = p1_x.reshape((len(p1_x), 1))
p2_x = p2_x.reshape((len(p2_x), 1))
p3_x = p3_x.reshape((len(p2_x), 2))


recon_1 = (x - x_mean) @ p1_x @ p1_x.T + x_mean
recon_2 = (x - x_mean) @ p2_x @ p2_x.T + x_mean
recon_3 = (x - x_mean) @ p3_x @ p3_x.T + x_mean

recon_e = np.linalg.norm(x - recon_1, ord = 2, axis = 1)
recon_e_s = np.sum(recon_e)
print(recon_e_s**2)
recon_e_2 = np.linalg.norm(x - recon_2, ord = 2, axis = 1)
recon_e_s_2 = np.sum(recon_e_2)
print(recon_e_s_2**2)

recon_e_3 = np.linalg.norm(x - recon_3, ord = 2, axis = 1)
recon_e_s_3 = np.sum(recon_e_3)
print(recon_e_s_3**2)

"""# Question 4: Eigenfaces

Now we will perform PCA on images of faces and see how reducing the dimensions of our images affects the images themselves. First start by uncommenting the code below to retrieve the faces dataset.
"""

"""
Load data (Faces Dataset).

Please remember to comment out this code before downloading your .py file.
"""

# from sklearn.datasets import fetch_olivetti_faces

# faces = fetch_olivetti_faces(shuffle=True, random_state = 10)

# """
# Preprocess data

# Please remember to comment out this code before downloading your .py file.
# """

# n_samples, h, w = faces.images.shape
# X_images = faces.data
# y_names = faces.target
# n_features = X_images.shape[1]

# print("Total dataset size:")
# print("n_samples: %d" % n_samples)
# print("n_features: %d" % n_features)

"""### PCA with SVD
PCA can also be implemented using Singular Value Decomposition on the data itself. Implement an alternative form of PCA below using SVD **Please call the np.linalg.svd function for SVD**.
"""

def pca2(X, n_pc):
  """
  Performs PCA on dataset X
  
  Args:
    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by
                          p (number of features) matrix                 
    n_pc: The number of principal components
  
  Returns:
    projected ((n, min(n, n_pc)) np.ndarray): Samples projected on reduced dimensions
    components ((n_pc, p) np.ndarray): Principal Components (also eigenvectors)
    values ((min(n, n_pc),) np.ndarray): Singular Values    
    mean ((p,) np.ndarray): Mean of features
    centered_data ((n, p) np.ndarray): Data with the mean subtracted
  """
  
  n, p = X.shape

  if n_pc > n:
    n_pc = n

  
  projected = np.zeros((n, n_pc))
  components = np.zeros((n_pc, p))
  values = np.zeros(n_pc,)
  mean = np.zeros(p,)
  centered_data = np.zeros((n,p))

  # <---- Start code here -----> 

  mean = np.mean(X, axis = 0)
  X_d = X - mean
  centered_data = X_d
  u, s, v_t = np.linalg.svd(X_d)


  v = v_t.T
  V = v[:, 0:n_pc]
  components = V.T
  projected = X_d @ V
  print(projected.shape)
  values = s[0:n_pc]


  # <---- End code here -----> 

  # pc 
  assert projected.shape == (n, min(n, n_pc))  
  assert components.shape == (n_pc, p)  
  assert values.shape == (min(n, n_pc),)
  assert mean.shape == (p,)  
  assert centered_data.shape == (n, p)    

  return projected, components, values, mean, centered_data

"""## Helper functions"""

"""It helps visualising the portraits from the dataset."""
def plot_portraits(images, titles, h, w, n_row, n_col):
  """
  Plots portraits of images
  
  Args:
    images ((n,p) np.ndarray): The input images in a vector format
    titles (list string): The list of names for each image
    h (int): The input height of the portraits
    w (int): The input height of the portraits
    n_row (int): The number of rows to display of images
    n_col (int): The number of columns to display of images
  """  
  
  plt.figure(figsize=(2.2 * n_col, 2.2 * n_row))
  plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.20)
  for i in range(n_row * n_col):
      plt.subplot(n_row, n_col, i + 1)
      plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
      plt.title(titles[i])
      plt.xticks(())
      plt.yticks(())

"""
Please remember to comment out this code before downloading your .py file.
"""

# plot_portraits(X_images, y_names, h, w, n_row=4, n_col=8)

"""Now run the script below that takes the image matrix of n rows by h*w features and runs PCA on this X with 50 components and plot the images of the eigenvectors (which you will have to unwrap back into the image shape.)"""

"""
Please remember to comment out this code before downloading your .py file.
"""

# n_components = 50
# P, C, V, M, Y = pca2(X_images, n_components)

# eigenfaces = C.reshape((n_components, h, w))
# eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
# plot_portraits(eigenfaces, eigenface_titles, h, w, 4, 8)

# # To report your singular values
# print(V)

"""### Question 4.1 Answer on Latex
Paste the eigenfaces on LaTeX. Please describe what the eigenfaces look like. Please describe what the eigenfaces look like. What do you expect to observe with the eigenfaces associated with lower eigenvalues?

## Reconstructing faces
Now implement a function to reconstruct a face given projected data points
"""

def reconstruction(P, C, M, h, w, image_index):
  """
  Reconstructs an image with the given components
  
  Args:
    P, projected ((n, n_pc) np.ndarray): Samples projected on reduced dimensions
    C, components ((n_pc, p) np.ndarray): Principal Components (also eigenvectors)
    M, mean ((p,) np.ndarray): Mean of features
    h (int): The input height of the portraits
    w (int): The input height of the portraits
    image_index (int):  The index of the image in the projected data matrix
    
  Returns:
    recovered_image ((h, w) np.ndarray): Reconstructed image from the inputs
  """    
  
  # <---- Start code here -----> 

  reconstruct = P[image_index,:] @ C # n x p
  M_ = M.reshape((1, len(M)))
  reconstrcut_shift = reconstruct + M_


  recovered_image = reconstrcut_shift.reshape((h, w))



  # <---- End code here -----> 
  
  return recovered_image

"""### Run the code below"""

"""
Please remember to comment out this code before downloading your .py file.
"""

# recovered_images=[reconstruction(P, C, M, h, w, i) for i in range(len(X_images))]
# plot_portraits(recovered_images, y_names, h, w, n_row=4, n_col=8)

"""### Question 4.2 Answer on Latex
Paste the portrait reconstructions on LaTeX.
1. Compare the reconstructed images to the original images. How are they similar and how are they different?
3. What do you expect to see from the reconstructed images as the number of principal components chosen for PCA decreases? Please explain why.
2. What do you expect to see from the reconstructed images as the number of principal components chosen for PCA increases? Please explain why.

## Explanation of Variance and Reconstruction Error

Make 3 plots. 
1. One with the reconstruction error over the number of components used. Plot the average of the reconstruction error for the first 50 images over reconstructions with varying number of components, normalized to the range of 0-1.
1. One with the eigenvalues from largest to smallest. 
1. And lastly, one with the cumulative (sum of) eigenvectors used per number of principal components (this is known as explained variance), normalized to the range of 0-1.
"""

"""
Please remember to comment out this code before downloading your .py file.
"""

# P, C, eig_vals, M, Y = pca2(X_images, n_features)

# reconst_errors = []
# for i in range(X_images.shape[0]):
#   dist = 0
#   for j in range(50):    
#     reconst_img = reconstruction(P[:, :i+1], C[:i+1, :], M, h, w, j)
#     dist += np.linalg.norm(np.array(reconst_img).flatten() - np.array(X_images[j]))
#   reconst_errors.append(dist/20)

"""
Plot reconstruction error for one image normalized.

Please remember to comment out this code before downloading your .py file.
"""

# plt.plot(reconst_errors/reconst_errors[0])

"""
Plot eigenvalues.

Please remember to comment out this code before downloading your .py file.
"""

# plt.plot(eig_vals)

"""
Plot cumulative eigenvalues over the number of components used.

Please remember to comment out this code before downloading your .py file.
"""

# tot = sum(eig_vals)
# var_exp = [(i / tot) for i in sorted(eig_vals, reverse=True)]
# cum_var_exp = np.cumsum(var_exp)

# plt.plot(cum_var_exp)

"""### Question 4.3 Answer on Latex
Paste the graphs into LaTeX.
1. How do you expect (based on theory; please be precise!) the plot of variance explained as the number of components to relate to the eigenvalues of the corresponding components?
1. What is the relation between reconstruction error and the variance explained?

# Turning it in


**Remember to recomment all script portions of this notebook before submitting (i.e. any code not in a function). This is to ensure that the Autograder works properly. Also make sure you did not edit other sections of the code outside of specified areas.**

1. Download this notebook as a `hw5.py` file with the functions implemented and other code commented out
  - Go to "File -> Download .py"
  
2. Submit `hw5.py` file to Gradescope (you can do this as many times as you'd like before the deadline)
"""